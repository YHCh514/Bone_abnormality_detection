{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from qqdm.notebook import qqdm\n",
    "\n",
    "def GetFileName(root):\n",
    "    filenames = glob.glob(os.path.join(root, 'HAND*_image1.png'))\n",
    "    #check if is nan\n",
    "    label = pd.read_csv(r'.\\data_path\\train.csv')\n",
    "    label = label.dropna()\n",
    "    label = label['id']\n",
    "    label = label.array\n",
    "    #delete nan hand\n",
    "    count = len(filenames) - 1\n",
    "    while (count >= 0):\n",
    "        if (filenames[count][-26:-11] not in label):\n",
    "            filenames.remove(filenames[count])\n",
    "        count = count - 1\n",
    "    return filenames\n",
    "\n",
    "class GetDataSet(Dataset):\n",
    "    def __init__(self,file,file_paths,train=True):\n",
    "        self.file_paths = file_paths #file names of images\n",
    "        self.train = train\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.RandomEqualize(1),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "        self.num_samples = len(self.file_paths)\n",
    "        self.filenames = glob.glob(os.path.join(os.path.join(r'.\\data_path',file), \"HAND*\"))\n",
    "        self.label = pd.read_csv(r'.\\data_path\\train.csv',index_col='id')\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        img1 = torchvision.io.read_image(file_path,torchvision.io.ImageReadMode(1))\n",
    "        img1 = self.transforms(img1)\n",
    "        check = 0\n",
    "        if(file_path[-len(file_path):-5]+'2.png' in self.filenames):\n",
    "            img2 = torchvision.io.read_image(file_path[-len(file_path):-5]+'2.png',torchvision.io.ImageReadMode(1))\n",
    "            img2 = self.transforms(img2)\n",
    "            check = check + 1\n",
    "        if(file_path[-len(file_path):-5]+'3.png' in self.filenames):\n",
    "            img3 = torchvision.io.read_image(file_path[-len(file_path):-5]+'3.png',torchvision.io.ImageReadMode(1))\n",
    "            img3 = self.transforms(img3)\n",
    "            check = check + 1\n",
    "        if(file_path[-len(file_path):-5]+'4.png' in self.filenames):\n",
    "            img4 = torchvision.io.read_image(file_path[-len(file_path):-5]+'4.png',torchvision.io.ImageReadMode(1))\n",
    "            img4 = self.transforms(img4)\n",
    "            check = check + 1\n",
    "        if (check == 3):img = torch.cat((img1,img2,img3,img4))\n",
    "        elif (check == 2):img = torch.cat((img1,img2,img3,img1))\n",
    "        elif (check == 1):img = torch.cat((img1,img2,img1,img1))\n",
    "        else:img = torch.cat((img1,img1,img1,img1))\n",
    "        \n",
    "        if (self.train):\n",
    "            return img,self.label.loc[file_path[-26:-11]][0]\n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = GetFileName(r'.\\data_path\\train')\n",
    "test_paths = glob.glob(os.path.join(r'.\\data_path\\test', 'HAND*_image1.png'))\n",
    "train_dataset = GetDataSet('train',train_paths,train = True)\n",
    "test_dataset = GetDataSet('test',test_paths,train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check dataset\n",
    "#the first index stands for the index of the images\n",
    "#the second index implies it is the image not the label\n",
    "#the third is the index of the channel\n",
    "img = train_dataset[0][0][0] \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = train_dataset[0][0][1]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = train_dataset[0][0][2]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = train_dataset[0][0][3]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct data loader\n",
    "import torch.utils.data as data\n",
    "batch_size = 8\n",
    "VAL_RATIO = 0.2\n",
    "percent = int(len(train_dataset) * (1 - VAL_RATIO))\n",
    "train_set, valid_set = data.random_split(train_dataset, [percent, len(train_dataset)-percent])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=valid_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get pretrained model using torchvision.models as models library\n",
    "from torch.autograd import Variable\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "#turn off training for their parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#change the number of input channels to 4 \n",
    "weight1 = model.conv1.weight.clone()\n",
    "new_first_layer  = nn.Conv2d(4, model.conv1.out_channels, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).requires_grad_()\n",
    "new_first_layer.weight[:,:3,:,:].data[...] =  Variable(weight1, requires_grad=True)\n",
    "model.conv1 = new_first_layer\n",
    "\n",
    "#check model weight\n",
    "print(model.conv1.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new classifier for model using torch.nn as nn library\n",
    "classifier_input = model.fc.in_features\n",
    "num_labels = 2 #PUT IN THE NUMBER OF LABELS IN YOUR DATA\n",
    "classifier = nn.Sequential(nn.Linear(classifier_input, 1024),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(1024, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(512, num_labels),\n",
    "                           nn.LogSoftmax(dim=1))\n",
    "#replace default classifier with new classifier\n",
    "model.fc = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the device available to use using torch library\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "# Move model to the device specified above\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the error function using torch.nn as nn library\n",
    "criterion = nn.NLLLoss()\n",
    "#set the optimizer function using torch.optim as optim library\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    # Training the model\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move to device\n",
    "        inputs, labels = inputs.to(device), labels.type(torch.ByteTensor).to(device)\n",
    "        # Clear optimizers\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "\n",
    "        output = model.forward(inputs)\n",
    "        # Loss\n",
    "        '''print(output,labels)'''\n",
    "        loss = criterion(output, labels)\n",
    "        # Calculate gradients (backpropogation)\n",
    "        loss.backward()\n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        # Add the loss to the training set's rnning loss\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "        \n",
    "        # Print the progress of our training\n",
    "        counter += 1\n",
    "        print(counter, \"/\", len(train_loader))\n",
    "        \n",
    "    # Evaluating the model\n",
    "    model.eval()\n",
    "    counter = 0\n",
    "    # Tell torch not to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.type(torch.ByteTensor).to(device)\n",
    "            # Forward pass\n",
    "            output = model.forward(inputs)\n",
    "            # Calculate Loss\n",
    "            valloss = criterion(output, labels)\n",
    "            # Add loss to the validation set's running loss\n",
    "            val_loss += valloss.item()*inputs.size(0)\n",
    "            \n",
    "            # Since our model outputs a LogSoftmax, find the real \n",
    "            # percentages by reversing the log function\n",
    "            output = torch.exp(output)\n",
    "            # Get the top class of the output\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            # See how many of the classes were correct?\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            # Calculate the mean (get the accuracy for this batch)\n",
    "            # and add it to the running accuracy for this epoch\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            \n",
    "            # Print the progress of our evaluation\n",
    "            counter += 1\n",
    "            print(counter, \"/\", len(val_loader))\n",
    "    \n",
    "    # Get the average loss for the entire epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = val_loss/len(val_loader.dataset)\n",
    "    # Print out the information\n",
    "    print('Accuracy: ', accuracy/len(val_loader))\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
